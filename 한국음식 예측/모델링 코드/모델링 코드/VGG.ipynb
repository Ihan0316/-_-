{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a42ee6b6b78fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 셀 1: 기본 설정 및 라이브러리 임포트\n",
    "# ======================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import (binary_crossentropy)\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "# 전역 정책을 'mixed_float16'으로 설정\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be9ad2db3ed8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 셀 2: 데이터셋 준비 및 모델 생성 함수 정의\n",
    "# ======================================================\n",
    "\n",
    "def prepare_datasets(base_path, seed, img_size, batch_size, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    지정된 경로에서 전체 데이터셋을 로드하고 훈련, 검증, 테스트용으로 분할합니다.\n",
    "    \"\"\"\n",
    "    print(\"전체 데이터셋 로딩 중...\")\n",
    "    full_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        base_path,\n",
    "        seed=seed,\n",
    "        image_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    class_names = full_dataset.class_names\n",
    "    print(f\"총 {len(class_names)}개의 클래스를 찾았습니다.\")\n",
    "\n",
    "    dataset_size = tf.data.experimental.cardinality(full_dataset).numpy()\n",
    "    train_size = int(train_ratio * dataset_size)\n",
    "    validation_size = int(val_ratio * dataset_size)\n",
    "\n",
    "    print(f\"전체: {dataset_size} 배치 -> 훈련: {train_size}, 검증: {validation_size}, 테스트: {dataset_size - train_size - validation_size} 배치로 분할\")\n",
    "\n",
    "    train_ds = full_dataset.take(train_size)\n",
    "    validation_ds = full_dataset.skip(train_size).take(validation_size)\n",
    "    test_ds = full_dataset.skip(train_size + validation_size)\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_ds = train_ds.shuffle(256).prefetch(buffer_size=AUTOTUNE)\n",
    "    validation_ds = validation_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    print(\"✅ 훈련, 검증, 테스트 데이터셋 분할 완료.\")\n",
    "    print(\"Found 150610 files belonging to 150 classes. ---- 이 문구와 다르다면 바로 말씀해주세요!!!!\")\n",
    "    return train_ds, validation_ds, test_ds, class_names\n",
    "\n",
    "def create_kfood_model(input_shape, num_classes):\n",
    "    base_model = tf.keras.applications.VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = preprocess_input(inputs)\n",
    "    x = base_model(x, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", dtype='float32')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a614f0faede96c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 셀 3: 메인 실행 - 데이터 준비, 모델 생성 및 1단계 학습\n",
    "# ======================================================\n",
    "\n",
    "seed = 53\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 파라미터 설정\n",
    "DATA_PATH = '../../데이터/data/kfood'\n",
    "MODEL_PATH = '../model'\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# --- 데이터 준비 ---\n",
    "train_dataset, validation_dataset, test_dataset, class_names = prepare_datasets(\n",
    "    base_path=DATA_PATH,\n",
    "    seed=seed,\n",
    "    img_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# --- 모델 생성 ---\n",
    "model = create_kfood_model(\n",
    "    input_shape=IMG_SIZE + (3,),\n",
    "    num_classes=len(class_names)\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# --- 1단계: 전이 학습 실행 ---\n",
    "print(\"\\n--- 1단계: 전이 학습 시작 ---\")\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\", restore_best_weights=True, verbose=1)\n",
    "ck_path = os.path.join(MODEL_PATH, \"best_kfood_model_vgg.keras\")\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "ck = tf.keras.callbacks.ModelCheckpoint(ck_path, save_best_only=True, monitor='val_loss', verbose=1)\n",
    "\n",
    "history_transfer = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=100,\n",
    "    callbacks=[es, ck]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d45e8da2eb8799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 셀 4: 1단계 학습 결과 시각화\n",
    "# ======================================================\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_transfer.history['loss'], label='Train Loss')\n",
    "plt.plot(history_transfer.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Transfer Learning - Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_transfer.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_transfer.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Transfer Learning - Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de700bd3148e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 셀 5: 2단계 - 미세 조정 실행\n",
    "# ======================================================\n",
    "print(\"\\n--- 2단계: 미세 조정 준비 ---\")\n",
    "\n",
    "# 가장 성능이 좋았던 모델을 로드\n",
    "model = tf.keras.models.load_model(ck_path)\n",
    "\n",
    "# 기반 모델 동결 해제 및 하위 레이어 재동결\n",
    "base_model = model.layers[2]\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 매우 낮은 학습률로 다시 컴파일\n",
    "fine_tune_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(\n",
    "    optimizer=fine_tune_optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n미세 조정을 위한 모델 구조:\")\n",
    "model.summary()\n",
    "\n",
    "# 미세 조정 학습 시작\n",
    "print(\"\\n--- 2단계: 미세 조정 시작 ---\")\n",
    "initial_epoch = history_transfer.epoch[-1] + 1\n",
    "history_fine = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=20,\n",
    "    initial_epoch=initial_epoch,\n",
    "    callbacks=[es, ck] # 동일한 콜백 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1309cc5d23c7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 셀 6: 최종 모델 평가\n",
    "# ======================================================\n",
    "print(\"\\n--- 최종 성능 평가 ---\")\n",
    "\n",
    "# 최종적으로 가장 성능이 좋았던 모델 로드\n",
    "best_model = tf.keras.models.load_model(ck_path)\n",
    "\n",
    "# 학습에 전혀 사용되지 않은 테스트 데이터셋으로 최종 성능 평가\n",
    "print(\"\\nTest 데이터셋으로 최종 성능을 평가합니다...\")\n",
    "loss, accuracy = best_model.evaluate(test_dataset)\n",
    "print(f\"최종 테스트 손실 (Final Test Loss): {loss:.4f}\")\n",
    "print(f\"최종 테스트 정확도 (Final Test Accuracy): {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-mps)",
   "language": "python",
   "name": "tf-mps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
